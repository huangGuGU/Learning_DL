{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0.weight True\n",
      "0.bias True\n",
      "1.weight True\n",
      "1.bias True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(8, 3, 50, 100)\n",
    "print(input.requires_grad)\n",
    "\n",
    "\n",
    "net = nn.Sequential(nn.Conv2d(3, 16, 3, 1),\n",
    "                    nn.Conv2d(16, 32, 3, 1))\n",
    "for param in net.named_parameters():\n",
    "    print(param[0], param[1].requires_grad)\n",
    "\n",
    "\n",
    "output = net(input)\n",
    "# 在写代码的过程中，不要把网络的输入和 Ground Truth 的 requires_grad 设置为 True。\n",
    "# 虽然这样设置不会影响反向传播，但是需要额外计算网络的输入和 Ground Truth 的导数，增大了计算量和内存占用不说，\n",
    "# 这些计算出来的导数结果也没啥用。因为我们只需要神经网络中的参数的导数，用来更新网络，其余的导数都不需要。\n",
    "\n",
    "print(output.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0.weight False\n",
      "0.bias False\n",
      "1.weight False\n",
      "1.bias False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(8, 3, 50, 100)\n",
    "print(input.requires_grad)\n",
    "\n",
    "net = nn.Sequential(nn.Conv2d(3, 16, 3, 1),\n",
    "                    nn.Conv2d(16, 32, 3, 1))\n",
    "for param in net.named_parameters():\n",
    "    param[1].requires_grad = False # 将梯度更新变成False，我们可以通过这种方法，在训练的过程中冻结部分网络，让这些层的参数不再更新，这在迁移学习中很有用处。\n",
    "    print(param[0], param[1].requires_grad)\n",
    "\n",
    "output = net(input)\n",
    "print(output.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 用一个新的 fc 层来取代之前的全连接层\n",
    "# 因为新构建的 fc 层的参数默认 requires_grad=True\n",
    "model.fc = nn.Linear(512, 100)\n",
    "\n",
    "# 只更新 fc 层的参数\n",
    "optimizer = optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9)\n",
    "\n",
    "# 通过这样，我们就冻结了 resnet 前边的所有层，\n",
    "# 在训练过程中只更新最后的 fc 层中的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 当我们在做 evaluating 的时候（不需要计算导数），我们可以将推断（inference）的代码包裹在 with torch.no_grad(): 之中，\n",
    "# 以达到 暂时 不追踪网络参数中的导数的目的，总之是为了减少可能存在的计算和内存消耗。\n",
    "x = torch.randn(3, requires_grad = True)\n",
    "print(x.requires_grad)\n",
    "# True\n",
    "print((x ** 2).requires_grad)\n",
    "# True\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)\n",
    "    # False\n",
    "\n",
    "print((x ** 2).requires_grad)\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True None\n",
      "False <AddBackward0 object at 0x1277489a0>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 在反向传播过程中，只有 is_leaf=True 的时候，需要求导的张量的导数结果才会被最后保留下来。对于 requires_grad=False 的 tensor 来说，我们约定俗成地把它们归为叶子张量。\n",
    "# 但其实无论如何划分都没有影响，因为张量的 is_leaf 属性只有在需要求导的时候才有意义。\n",
    "\n",
    "\n",
    "\n",
    "# 我们真正需要注意的是当 requires_grad=True 的时候，如何判断是否是叶子张量：\n",
    "\n",
    "a = torch.ones([2, 2], requires_grad=True)\n",
    "print(a.is_leaf,a.grad_fn)\n",
    "# 当这个 tensor 是用户创建的时候，它是一个叶子节点,对于叶子节点来说，它们的 grad_fn 属性都为空；\n",
    "\n",
    "b = a + 2\n",
    "print(b.is_leaf,b.grad_fn)\n",
    "\n",
    "# 当这个 tensor 是由其他运算操作产生的时候，它就不是一个叶子节点,因为 b 不是用户创建的，是通过计算生成的\n",
    "#  而对于非叶子结点来说，因为它们是通过一些操作生成的，所以它们的 grad_fn 不为空。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5121600144\n",
      "5121600864\n",
      "5100578112\n",
      "5100578112 tensor([10.,  1.])\n"
     ]
    }
   ],
   "source": [
    "'''inplace操作'''\n",
    "# 情景 1\n",
    "a = torch.tensor([3.0, 1.0])\n",
    "print(id(a))\n",
    "a = a.exp()\n",
    "print(id(a))\n",
    "# 在这个过程中 a.exp() 生成了一个新的对象，然后再让 a\n",
    "# 指向它的地址，所以这不是个 inplace 操作\n",
    "\n",
    "# 情景 2\n",
    "a = torch.tensor([3.0, 1.0])\n",
    "print(id(a)) \n",
    "a[0] = 10\n",
    "print(id(a), a) # 2112716403840 tensor([10.,  1.])\n",
    "# inplace 操作，内存地址没变\n",
    "\n",
    "# 所以类似 i=i+1 不是inplace操作，而i+=1是inplace操作\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch通过._version来判断是否是inplace操作\n",
    "# 每次 tensor 在进行 inplace 操作时，变量 _version 就会加1，其初始值为0。\n",
    "# 在正向传播过程中，求导系统记录的 b 的 version 是0，但是在进行反向传播的过程中，求导系统发现 b 的 version 变成1了，所以就会报错了。\n",
    "# 但是还有一种特殊情况不会报错，就是反向传播求导的时候如果没用到 b 的值（比如 y=x+1， y 关于 x 的导数是1，和 x 无关），自然就不会去对比 b 前后的 version 了，所以不会报错。\n",
    "\n",
    "\n",
    "a = torch.tensor([1.0, 3.0], requires_grad=True)\n",
    "b = a + 2\n",
    "print(b._version) # 0\n",
    "\n",
    "loss = (b * b).mean()\n",
    "b[0] = 1000.0\n",
    "print(b._version) # 1\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.,  5.,  2.,  3.], requires_grad=True) True\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a view of a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m/Users/hzh/Library/Mobile Documents/com~apple~CloudDocs/Python/学习程序/知识点/Learn_tensor知识.ipynb Cell 9'\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/hzh/Library/Mobile%20Documents/com~apple~CloudDocs/Python/%E5%AD%A6%E4%B9%A0%E7%A8%8B%E5%BA%8F/%E7%9F%A5%E8%AF%86%E7%82%B9/Learn_tensor%E7%9F%A5%E8%AF%86.ipynb#ch0000020?line=4'>5</a>\u001B[0m \u001B[39mprint\u001B[39m(a, a\u001B[39m.\u001B[39mis_leaf)\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/hzh/Library/Mobile%20Documents/com~apple~CloudDocs/Python/%E5%AD%A6%E4%B9%A0%E7%A8%8B%E5%BA%8F/%E7%9F%A5%E8%AF%86%E7%82%B9/Learn_tensor%E7%9F%A5%E8%AF%86.ipynb#ch0000020?line=5'>6</a>\u001B[0m \u001B[39m# tensor([10.,  5.,  2.,  3.], requires_grad=True) True\u001B[39;00m\n\u001B[0;32m----> <a href='vscode-notebook-cell:/Users/hzh/Library/Mobile%20Documents/com~apple~CloudDocs/Python/%E5%AD%A6%E4%B9%A0%E7%A8%8B%E5%BA%8F/%E7%9F%A5%E8%AF%86%E7%82%B9/Learn_tensor%E7%9F%A5%E8%AF%86.ipynb#ch0000020?line=7'>8</a>\u001B[0m a[:] \u001B[39m=\u001B[39m \u001B[39m0\u001B[39m\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/hzh/Library/Mobile%20Documents/com~apple~CloudDocs/Python/%E5%AD%A6%E4%B9%A0%E7%A8%8B%E5%BA%8F/%E7%9F%A5%E8%AF%86%E7%82%B9/Learn_tensor%E7%9F%A5%E8%AF%86.ipynb#ch0000020?line=8'>9</a>\u001B[0m \u001B[39mprint\u001B[39m(a, a\u001B[39m.\u001B[39mis_leaf)\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/hzh/Library/Mobile%20Documents/com~apple~CloudDocs/Python/%E5%AD%A6%E4%B9%A0%E7%A8%8B%E5%BA%8F/%E7%9F%A5%E8%AF%86%E7%82%B9/Learn_tensor%E7%9F%A5%E8%AF%86.ipynb#ch0000020?line=9'>10</a>\u001B[0m \u001B[39m# tensor([0., 0., 0., 0.], grad_fn=<CopySlices>) False\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: a view of a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "# 如果是grad_required = True 的叶子节点，inplace操作则会报错\n",
    "# 本来是该有导数值保留的变量，现在成了导数会被自动释放的中间变量了，所以 PyTorch 就给你报错了。\n",
    "\n",
    "a = torch.tensor([10., 5., 2., 3.], requires_grad=True)\n",
    "print(a, a.is_leaf)\n",
    "# tensor([10.,  5.,  2.,  3.], requires_grad=True) True\n",
    "\n",
    "a[:] = 0\n",
    "print(a, a.is_leaf)\n",
    "# tensor([0., 0., 0., 0.], grad_fn=<CopySlices>) False\n",
    "\n",
    "loss = (a*a).mean()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m/Users/hzh/Library/Mobile Documents/com~apple~CloudDocs/Python/学习程序/知识点/Learn_tensor知识.ipynb Cell 10'\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/hzh/Library/Mobile%20Documents/com~apple~CloudDocs/Python/%E5%AD%A6%E4%B9%A0%E7%A8%8B%E5%BA%8F/%E7%9F%A5%E8%AF%86%E7%82%B9/Learn_tensor%E7%9F%A5%E8%AF%86.ipynb#ch0000021?line=0'>1</a>\u001B[0m a \u001B[39m=\u001B[39m torch\u001B[39m.\u001B[39mtensor([\u001B[39m10.\u001B[39m, \u001B[39m5.\u001B[39m, \u001B[39m2.\u001B[39m, \u001B[39m3.\u001B[39m], requires_grad\u001B[39m=\u001B[39m\u001B[39mTrue\u001B[39;00m)\n\u001B[0;32m----> <a href='vscode-notebook-cell:/Users/hzh/Library/Mobile%20Documents/com~apple~CloudDocs/Python/%E5%AD%A6%E4%B9%A0%E7%A8%8B%E5%BA%8F/%E7%9F%A5%E8%AF%86%E7%82%B9/Learn_tensor%E7%9F%A5%E8%AF%86.ipynb#ch0000021?line=1'>2</a>\u001B[0m a\u001B[39m.\u001B[39;49madd_(\u001B[39m10.\u001B[39;49m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "# 同样grad_required = True 的叶子节点，只要你对需要求导的叶子张量使用了这些操作，不需等到反向传播，马上就会报错。\n",
    "a = torch.tensor([10., 5., 2., 3.], requires_grad=True)\n",
    "a.add_(10.) # 或者 a += 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''给叶子节点赋值的正确方法'''\n",
    "#PyTorch 不推荐使用 inplace 操作，当求导过程中发现有 inplace 操作影响求导正确性的时候，会采用报错的方式提醒。\n",
    "# 但这句话反过来说就是，因为只要有 inplace 操作不当就会报错，所以如果我们在程序中使用了 inplace 操作却没报错，那么说明我们最后求导的结果是正确的，没问题的。这就是我们常听见的没报错就没有问题。\n",
    "\n",
    "\n",
    "\n",
    "# 方法一\n",
    "a = torch.tensor([10., 5., 2., 3.], requires_grad=True)\n",
    "print(a, a.is_leaf, id(a))\n",
    "# tensor([10.,  5.,  2.,  3.], requires_grad=True) True 2501274822696\n",
    "\n",
    "a.data.fill_(10.)\n",
    "# 或者 a.detach().fill_(10.)\n",
    "print(a, a.is_leaf, id(a))\n",
    "# tensor([10., 10., 10., 10.], requires_grad=True) True 2501274822696\n",
    "\n",
    "loss = (a*a).mean()\n",
    "loss.backward()\n",
    "print(a.grad)\n",
    "# tensor([5., 5., 5., 5.])\n",
    "\n",
    "\n",
    "\n",
    "# 方法二\n",
    "a = torch.tensor([10., 5., 2., 3.], requires_grad=True)\n",
    "print(a, a.is_leaf)\n",
    "# tensor([10.,  5.,  2.,  3.], requires_grad=True) True\n",
    "\n",
    "with torch.no_grad():\n",
    "    a[:] = 10.\n",
    "print(a, a.is_leaf)\n",
    "# tensor([10., 10., 10., 10.], requires_grad=True) True\n",
    "\n",
    "loss = (a*a).mean()\n",
    "loss.backward()\n",
    "print(a.grad)\n",
    "# tensor([5., 5., 5., 5.])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9., 2., 2.], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "'''detach '''\n",
    "a = torch.tensor([7., 0, 0], requires_grad=True)\n",
    "b = a + 2\n",
    "print(b)\n",
    "# tensor([9., 2., 2.], grad_fn=<AddBackward0>)\n",
    "\n",
    "loss = torch.mean(b * b)\n",
    "\n",
    "b_ = b.detach() # 可以得到 tensor的数据 + requires_grad=False 的版本\n",
    "b_.zero_()\n",
    "print(b)\n",
    "# tensor([0., 0., 0.], grad_fn=<AddBackward0>)\n",
    "# 储存空间共享，修改 b_ , b 的值也变了\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#关于使用 GPU 还有一个点，在我们想把 GPU tensor 转换成 Numpy 变量的时候，需要先将 tensor 转换到 CPU 中去，因为 Numpy 是 CPU-only 的。\n",
    "# x  = torch.rand([3,3], device='cuda')\n",
    "# x_ = x.cpu().numpy()\n",
    "\n",
    "#其次，如果 tensor 需要求导的话，还需要加一步 detach，再转成 Numpy 。\n",
    "# y  = torch.rand([3,3], requires_grad=True, device='cuda')\n",
    "# y_ = y.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4484], requires_grad=True)\n",
      "0.4484216272830963 <class 'float'>\n",
      "[[-0.9549000859260559, 1.7563014030456543], [-0.007843704894185066, -1.2936357259750366]]\n"
     ]
    }
   ],
   "source": [
    "''' tensor.item()'''\n",
    "# item() 只适用于 tensor 只包含一个元素的时候。因为大多数情况下我们的 loss 就只有一个元素，所以就经常会用到 loss.item()。\n",
    "x  = torch.randn(1, requires_grad=True, device='cpu')\n",
    "print(x)\n",
    "y = x.item()\n",
    "print(y, type(y))\n",
    "\n",
    "\n",
    "# 如果想把含多个元素的 tensor 转换成 Python list 的话，要使用 tensor.tolist()\n",
    "x = torch.randn([2, 2])\n",
    "y = x.tolist()\n",
    "print(y)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c98f917436e91ad7f0c3d4f5d9b3df817d2512acd5c63c803caddd09106776b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 ('tfpy39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

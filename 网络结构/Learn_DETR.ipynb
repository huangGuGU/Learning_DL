{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 网络结构\n",
    "\n",
    "传统的方法都需要用间接的方式去处理，RCNN 系列用 proposals 的方式，YOLO 用的anchor，windows center 的方式。很多性能好的检测器新能都限制于后处理nms 的操作（因为同一个物体会出现很多大大小小的框，需要用 nms 抑制），就是因为有了这一部，所以会让这些检测器更加复杂难以优化，所以 DETR 是用一种直接的端到端的方式，之前有这样想法的模型但是以前的尝试要么添加其他形式的先验知识，要么没有证明在具有挑战性的基准上与强基线具有竞争力。\n",
    "\n",
    "\n",
    "\n",
    "Transformer 的自注意力机制显式地对序列中元素之间的所有成对交互进行建模，使这些体系结构特别适用于集合预测的特定约束，例如删除重复预测。\n",
    "\n",
    "DEtection TRAnsformer（DETR）一次预测所有对象，并使用一组损失函数进行端到端训练，该损失函数在预测对象和真实对象之间执行二分匹配\n",
    "\n",
    "与大多数现有检测方法不同，DETR 不需要任何自定义层，因此可以在任何包含标准 CNN 和转换器类的框架中轻松复制。\n",
    "\n",
    "![image-20230412上午103443716](../配图/Transformer家族.assets/image-20230412上午103443716.png)\n",
    "\n",
    "\n",
    "\n",
    "![image-20230412上午112545489](../配图/Transformer家族.assets/image-20230412上午112545489.png)\n",
    "\n",
    "- 实例\n",
    "\n",
    "输入(3,800,1066)，经过卷积层后变成(2048,25,34)，因为要进入 transformer，所以进行降维变成(256,25,34)，加上同样尺寸的位置编码是固定值。\n",
    "\n",
    "拉直变成（850,256）进入 transformer，文中使用了 6 个 encoder，输出保持尺寸不变。\n",
    "\n",
    "decoder 有一个 object queries (100,256)，用encoder 过来的（850,256）和object queries做自注意力特征，目的是移除冗余框，经过 6 个 decoder，最终输出一个(100,256)，作者在每个 decoder 都接一个检测头去计算 loss。\n",
    "\n",
    "检测头就是几个 FFN，进行类别分类 coco 就是 91 个类，然后进行100 个框的输出，用匈牙利算法进行框的匹配。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "\"\"\"\n",
    "DETR Transformer class.\n",
    "\n",
    "Copy-paste from torch.nn.Transformer with modifications:\n",
    "    * positional encodings are passed in MHattention\n",
    "    * extra LN at the end of encoder is removed\n",
    "    * decoder returns a stack of activations from all decoding layers\n",
    "\"\"\"\n",
    "import copy\n",
    "from typing import Optional, List\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=256, nhead=4, num_encoder_layers=3,\n",
    "                 num_decoder_layers=3, dim_feedforward=1024, dropout=0.1,\n",
    "                 activation=\"relu\", normalize_before=False,\n",
    "                 return_intermediate_dec=False):\n",
    "        super().__init__()\n",
    "\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,\n",
    "                                                dropout, activation, normalize_before)\n",
    "        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward,\n",
    "                                                dropout, activation, normalize_before)\n",
    "        decoder_norm = nn.LayerNorm(d_model)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm,\n",
    "                                          return_intermediate=return_intermediate_dec)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, src, mask, query_embed, pos_embed):\n",
    "        # flatten NxCxHxW to HWxNxC\n",
    "        bs, c, h, w = src.shape\n",
    "        src = src.flatten(2).permute(2, 0, 1)\n",
    "        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n",
    "        query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)\n",
    "        mask = mask.flatten(1)\n",
    "\n",
    "        tgt = torch.zeros_like(query_embed)\n",
    "        memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)\n",
    "        hs = self.decoder(tgt, memory, memory_key_padding_mask=mask,\n",
    "                          pos=pos_embed, query_pos=query_embed)\n",
    "        return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h, w)\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src,\n",
    "                mask: Optional[Tensor] = None,\n",
    "                src_key_padding_mask: Optional[Tensor] = None,\n",
    "                pos: Optional[Tensor] = None):\n",
    "        output = src\n",
    "\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, src_mask=mask,\n",
    "                           src_key_padding_mask=src_key_padding_mask, pos=pos)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "        self.return_intermediate = return_intermediate\n",
    "\n",
    "    def forward(self, tgt, memory,\n",
    "                tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                pos: Optional[Tensor] = None,\n",
    "                query_pos: Optional[Tensor] = None):\n",
    "        output = tgt\n",
    "\n",
    "        intermediate = []\n",
    "\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, memory, tgt_mask=tgt_mask,\n",
    "                           memory_mask=memory_mask,\n",
    "                           tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                           memory_key_padding_mask=memory_key_padding_mask,\n",
    "                           pos=pos, query_pos=query_pos)\n",
    "            if self.return_intermediate:\n",
    "                intermediate.append(self.norm(output))\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "            if self.return_intermediate:\n",
    "                intermediate.pop()\n",
    "                intermediate.append(output)\n",
    "\n",
    "        if self.return_intermediate:\n",
    "            return torch.stack(intermediate)\n",
    "\n",
    "        return output.unsqueeze(0)\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
    "                 activation=\"relu\", normalize_before=False):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.normalize_before = normalize_before\n",
    "\n",
    "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    def forward_post(self,\n",
    "                     src,\n",
    "                     src_mask: Optional[Tensor] = None,\n",
    "                     src_key_padding_mask: Optional[Tensor] = None,\n",
    "                     pos: Optional[Tensor] = None):\n",
    "        q = k = self.with_pos_embed(src, pos)\n",
    "        src2 = self.self_attn(q, k, value=src, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "    def forward_pre(self, src,\n",
    "                    src_mask: Optional[Tensor] = None,\n",
    "                    src_key_padding_mask: Optional[Tensor] = None,\n",
    "                    pos: Optional[Tensor] = None):\n",
    "        src2 = self.norm1(src)\n",
    "        q = k = self.with_pos_embed(src2, pos)\n",
    "        src2 = self.self_attn(q, k, value=src2, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src2 = self.norm2(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        return src\n",
    "\n",
    "    def forward(self, src,\n",
    "                src_mask: Optional[Tensor] = None,\n",
    "                src_key_padding_mask: Optional[Tensor] = None,\n",
    "                pos: Optional[Tensor] = None):\n",
    "        if self.normalize_before:\n",
    "            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)\n",
    "        return self.forward_post(src, src_mask, src_key_padding_mask, pos)\n",
    "\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
    "                 activation=\"relu\", normalize_before=False):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.normalize_before = normalize_before\n",
    "\n",
    "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    def forward_post(self, tgt, memory,\n",
    "                     tgt_mask: Optional[Tensor] = None,\n",
    "                     memory_mask: Optional[Tensor] = None,\n",
    "                     tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                     memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                     pos: Optional[Tensor] = None,\n",
    "                     query_pos: Optional[Tensor] = None):\n",
    "        q = k = self.with_pos_embed(tgt, query_pos)\n",
    "        tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n",
    "                                   key=self.with_pos_embed(memory, pos),\n",
    "                                   value=memory, attn_mask=memory_mask,\n",
    "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "    def forward_pre(self, tgt, memory,\n",
    "                    tgt_mask: Optional[Tensor] = None,\n",
    "                    memory_mask: Optional[Tensor] = None,\n",
    "                    tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                    memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                    pos: Optional[Tensor] = None,\n",
    "                    query_pos: Optional[Tensor] = None):\n",
    "        tgt2 = self.norm1(tgt)\n",
    "        q = k = self.with_pos_embed(tgt2, query_pos)\n",
    "        tgt2 = self.self_attn(q, k, value=tgt2, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt2 = self.norm2(tgt)\n",
    "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),\n",
    "                                   key=self.with_pos_embed(memory, pos),\n",
    "                                   value=memory, attn_mask=memory_mask,\n",
    "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt2 = self.norm3(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        return tgt\n",
    "\n",
    "    def forward(self, tgt, memory,\n",
    "                tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                pos: Optional[Tensor] = None,\n",
    "                query_pos: Optional[Tensor] = None):\n",
    "        if self.normalize_before:\n",
    "            return self.forward_pre(tgt, memory, tgt_mask, memory_mask,\n",
    "                                    tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)\n",
    "        return self.forward_post(tgt, memory, tgt_mask, memory_mask,\n",
    "                                 tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)\n",
    "\n",
    "\n",
    "def _get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    \"\"\"Return an activation function given a string\"\"\"\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    if activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    if activation == \"glu\":\n",
    "        return F.glu\n",
    "    raise RuntimeError(F\"activation should be relu/gelu, not {activation}.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    d_model = 256\n",
    "    model = Transformer(d_model=d_model, nhead=4, num_encoder_layers=3,\n",
    "                        num_decoder_layers=3, dim_feedforward=1024, dropout=0.1,\n",
    "                        activation=\"relu\", normalize_before=False,\n",
    "                        return_intermediate_dec=False)\n",
    "    n = 25\n",
    "    m = 34\n",
    "    batch = 16\n",
    "    query_num = 100\n",
    "    data = torch.rand(batch, d_model, n, m)\n",
    "    mask = torch.rand(batch, n * m)\n",
    "    query_e = torch.rand(query_num, d_model)\n",
    "    pos_e = torch.rand(batch, d_model, n, m)\n",
    "    output = model(data, mask, query_e, pos_e)\n",
    "    pass\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T14:18:02.469871Z",
     "end_time": "2023-04-18T14:18:09.689385Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 损失函数\n",
    "\n",
    "DETR 每次都会输出 100 个框，通过二分图匹配的方式找到100 个框里最合适的和 label 的框进行匹配。\n",
    "\n",
    "\n",
    "\n",
    "损失函数就是用分类的 loss 和框的 loss 组合，因为想让两个loss 在同一个空间，搜易分类loss 不计算 log；\n",
    "\n",
    "![image-20230412上午112100555](../配图/Transformer家族.assets/image-20230412上午112100555.png)\n",
    "\n",
    "\n",
    "\n",
    "- Box loss\n",
    "\n",
    "  因为 DETR 适合检测大目标，框的 loss 使用了 L1 和 generousiou loss使得画的框即使大，loss 也不会太大，$\\lambda_{iou}$ 和 $\\lambda_{L1}$ 是两个超参数 $L_{iou}$ 是generalized IoU。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  - **L1正则化的loss**\n",
    "\n",
    "    ```python\n",
    "    cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\n",
    "    ```\n",
    "\n",
    "\n",
    "\n",
    "  - **iou 的loss**\n",
    "\n",
    "    B 代表的是最大框，并集或交集的面积计算 $b_{\\sigma(i)}$ 和 $\\hat{b}_i$ 的线性函数的最小值/最大值 ，这使得对于sgd，损失表现得足够好。\n",
    "\n",
    "    ![image-20230412下午50411297](../配图/Transformer家族.assets/image-20230412下午50411297.png)\n",
    "\n",
    "\n",
    "\n",
    "    ```python\n",
    "    def box_iou(boxes1, boxes2):\n",
    "        area1 = box_area(boxes1)\n",
    "        area2 = box_area(boxes2)\n",
    "\n",
    "        lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n",
    "        rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n",
    "\n",
    "        wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
    "        inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
    "\n",
    "        union = area1[:, None] + area2 - inter\n",
    "\n",
    "        iou = inter / union\n",
    "        return iou, union\n",
    "\n",
    "    def generalized_box_iou(boxes1, boxes2):\n",
    "        \"\"\"\n",
    "        Generalized IoU from https://giou.stanford.edu/\n",
    "\n",
    "        The boxes should be in [x0, y0, x1, y1] format\n",
    "\n",
    "        Returns a [N, M] pairwise matrix, where N = len(boxes1)\n",
    "        and M = len(boxes2)\n",
    "        \"\"\"\n",
    "        # degenerate boxes gives inf / nan results\n",
    "        # so do an early check\n",
    "        assert (boxes1[:, 2:] >= boxes1[:, :2]).all()\n",
    "        assert (boxes2[:, 2:] >= boxes2[:, :2]).all()\n",
    "        iou, union = box_iou(boxes1, boxes2)\n",
    "\n",
    "        lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n",
    "        rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n",
    "\n",
    "        wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
    "        area = wh[:, :, 0] * wh[:, :, 1]\n",
    "\n",
    "        return iou - (area - union) / area\n",
    "    ```\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-18T14:18:09.689650Z",
     "end_time": "2023-04-18T14:18:09.691857Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
